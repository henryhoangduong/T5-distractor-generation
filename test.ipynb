{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Projects/fine-tuning/T5-distractor-generation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-distractor-model\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-distractor-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    correct_idx = ord(example[\"answer\"]) - ord(\"A\")\n",
    "    correct = example[\"options\"][correct_idx]\n",
    "    distractors = [opt for i, opt in enumerate(\n",
    "        example[\"options\"]) if i != correct_idx]\n",
    "\n",
    "    return {\n",
    "        \"input_text\": f\"generate distractors: question: {example['question']} context: {example['article']} answer: {correct}\",\n",
    "        \"target_text\": \" | \".join(distractors)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distractors(example, max_length=100000):\n",
    "    # Use the same preprocessing logic as during training\n",
    "    data = preprocess(example)\n",
    "    inputs = tokenizer(data[\"input_text\"],\n",
    "                       return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded.split(\" | \")  # Returns list of distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Distractors: [\"generate distractors: question: In which country is the prom called a 'formal'? context: It is well-known that the 'prom', a formal dance held at the end of high school or college, is an important date in every student's life. What is less well-known, however, is that in some countries, it goes by different names. For example, in Australia, it is called a 'formal'. answer: Australia\"]\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "    \"question\": \"In which country is the prom called a 'formal'?\",\n",
    "    \"article\": \"It is well-known that the 'prom', a formal dance held at the end of high school or college, is an important date in every student's life. What is less well-known, however, is that in some countries, it goes by different names. For example, in Australia, it is called a 'formal'.\",\n",
    "    \"options\": [\"USA\", \"UK\", \"Australia\", \"Canada\"],\n",
    "    \"answer\": \"C\"\n",
    "}\n",
    "\n",
    "distractors = generate_distractors(example)\n",
    "print(\"Generated Distractors:\", distractors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    correct_idx = ord(example[\"answer\"]) - ord(\"A\")\n",
    "    correct = example[\"options\"][correct_idx]\n",
    "    distractors = [opt for i, opt in enumerate(\n",
    "        example[\"options\"]) if i != correct_idx]\n",
    "\n",
    "    return {\n",
    "        \"input_text\": f\"generate distractors: question: {example['question']} context: {example['article']} answer: {correct}\",\n",
    "        \"target_text\": \" | \".join(distractors)\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_distractors(example, model, tokenizer, max_length=64):\n",
    "    model.eval()\n",
    "\n",
    "    data = preprocess(example)\n",
    "    input_text = data[\"input_text\"]\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded.split(\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Distractors: [\"generate distractors: question: In which country is the prom called a 'formal'? context: It is well-known that the 'prom', a formal dance held at the end of high school or college, is an important date in every student's life. What is\"]\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "    \"question\": \"In which country is the prom called a 'formal'?\",\n",
    "    \"article\": \"It is well-known that the 'prom', a formal dance held at the end of high school or college, is an important date in every student's life. What is less well-known, however, is that in some countries, it goes by different names. For example, in Australia, it is called a 'formal'.\",\n",
    "    \"options\": [\"USA\", \"UK\", \"Australia\", \"Canada\"],\n",
    "    \"answer\": \"C\"\n",
    "}\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-distractor-model\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-distractor-model\")\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Generate distractors\n",
    "distractors = generate_distractors(example, model, tokenizer)\n",
    "print(\"Generated Distractors:\", distractors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated distractors: generate distractors: question: What is business analytics primarily used for? context: Business analytics is the use of data, IT, statistics, and models to inform business decisions. answer: To make better decisions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"t5-distractor-model\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Example input\n",
    "question = \"What is business analytics primarily used for?\"\n",
    "context = \"Business analytics is the use of data, IT, statistics, and models to inform business decisions.\"\n",
    "answer = \"To make better decisions\"\n",
    "\n",
    "# Format the input as during training\n",
    "input_text = f\"generate distractors: question: {question} context: {context} answer: {answer}\"\n",
    "\n",
    "# Tokenize and prepare the input\n",
    "input_ids = tokenizer.encode(\n",
    "    input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(input_ids=input_ids, max_length=64,\n",
    "                         num_beams=5, early_stopping=True)\n",
    "\n",
    "# Decode and print\n",
    "distractors = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated distractors:\", distractors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated distractors: To make better decisions\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model_path = \"t5-distractor-model\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Create an example input\n",
    "question = \"What is business analytics primarily used for?\"\n",
    "context = \"Business analytics is the use of data, IT, statistics, and models to inform business decisions.\"\n",
    "answer = \"To make better decisions\"\n",
    "\n",
    "# Format input as used during training\n",
    "input_text = f\"generate distractors: question: {question} context: {context} answer: {answer}\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\",\n",
    "                   truncation=True, max_length=512).to(device)\n",
    "\n",
    "# Generate prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=64,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "distractors = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated distractors:\", distractors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated distractors:\n",
      "- False\n",
      "- True\n",
      "- Not_entailment\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"t5-base\"\n",
    "checkpoint_path = \"model_weights/t5-base-Race-Distractor-Generation-version0-step0.pt\"\n",
    "max_length = 512\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, model_max_length=max_length)\n",
    "tokenizer.add_special_tokens({\"sep_token\": \"<sep>\"})\n",
    "model.resize_token_embeddings(32128)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(32128)\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model'])  # ⬅️ THIS is the fix\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "question = \"What is the main function of mitochondria in a cell?\"\n",
    "context = \"Mitochondria are known as the powerhouses of the cell because they produce energy through cellular respiration. They convert glucose and oxygen into ATP, the energy currency of the cell.\"\n",
    "answer = \"To produce energy\"\n",
    "test_input = f\"generate distractors: {question} context: {context} answer: {answer}\"\n",
    "\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\",\n",
    "                   truncation=True, padding=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=64,\n",
    "        num_beams=5,\n",
    "        num_return_sequences=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "decoded = [tokenizer.decode(output, skip_special_tokens=True)\n",
    "           for output in outputs]\n",
    "print(\"Generated distractors:\")\n",
    "for d in decoded:\n",
    "    print(\"-\", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is the best title for the passage?\n",
      "answer:  Djokovic's application for special permission to enter the United States\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"potsawee/t5-large-generation-race-QuestionAnswer\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"potsawee/t5-large-generation-race-QuestionAnswer\")\n",
    "\n",
    "context = r\"\"\"\n",
    "World number one Novak Djokovic says he is hoping for a \"positive decision\" to allow him \n",
    "to play at Indian Wells and the Miami Open next month. The United States has extended \n",
    "its requirement for international visitors to be vaccinated against Covid-19. Proof of vaccination \n",
    "will be required to enter the country until at least 10 April, but the Serbian has previously \n",
    "said he is unvaccinated. The 35-year-old has applied for special permission to enter the country. \n",
    "Indian Wells and the Miami Open - two of the most prestigious tournaments on the tennis calendar \n",
    "outside the Grand Slams - start on 6 and 20 March respectively. Djokovic says he will return to \n",
    "the ATP tour in Dubai next week after claiming a record-extending 10th Australian Open title \n",
    "and a record-equalling 22nd Grand Slam men's title last month.\"\"\".replace(\"\\n\", \"\")\n",
    "\n",
    "inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "question_answer = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "question_answer = question_answer.replace(\n",
    "    tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\n",
    "question, answer = question_answer.split(tokenizer.sep_token)\n",
    "\n",
    "print(\"question:\", question)\n",
    "print(\"answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t5-distractor-generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
